# ===============================
# ğŸ“Œ Kalp HastalÄ±ÄŸÄ± Tahmini Projesi - AÅŸama 1
# ===============================

# Gerekli kÃ¼tÃ¼phanelerin yÃ¼klenmesi
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Makine Ã¶ÄŸrenmesi iÃ§in
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# KullanÄ±lacak algoritmalar
#from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier


# ===============================
# ğŸ“‚ 1. Veri Setinin YÃ¼klenmesi
# ===============================

# CSV dosyasÄ±nÄ± oku
df = pd.read_csv("heart.csv")  # Dosya ismini doÄŸru yazdÄ±ÄŸÄ±ndan emin ol
print("\nğŸ“Œ Ä°lk 5 SatÄ±r:")
print(df.head())


# ===============================
# ğŸ§  2. Ã–zellik MÃ¼hendisliÄŸi
# ===============================
# Yeni kombinasyon Ã¶znitelikler
df['age_chol'] = df['age'] * df['chol']
df['ecg_thalach'] = df['restecg'] * df['thalach']
df['slope_ca'] = df['slope'] * df['ca']

# Yeni Ã¶zellikleri gÃ¶rmek iÃ§in (opsiyonel)
print("\nğŸ§¬ Yeni Ã–zellikler Eklendi:")
print(df[['age_chol', 'ecg_thalach', 'slope_ca']].head())

# ===============================
# ğŸ” 3. Temel Bilgilerin Ä°ncelenmesi
# ===============================

print("\nğŸ” Veri Seti Bilgileri:")
print(df.info())

print("\nğŸ“Š Temel Ä°statistikler:")
print(df.describe())
print("\nğŸ“‹ Veri setindeki tÃ¼m sÃ¼tunlar:")
print(df.columns.tolist())


print("\nâ“ Eksik Veri KontrolÃ¼:")
print(df.isnull().sum())

# ===============================
# â¤ï¸ 4. Hedef DeÄŸiÅŸken DaÄŸÄ±lÄ±mÄ±
# ===============================

sns.countplot(x='target', data=df)
plt.title("Kalp HastalÄ±ÄŸÄ± DaÄŸÄ±lÄ±mÄ± (0 = Yok, 1 = Var)")
plt.xlabel("Kalp HastalÄ±ÄŸÄ±")
plt.ylabel("KiÅŸi SayÄ±sÄ±")
plt.show()

print("\nğŸ“Š Hedef DeÄŸiÅŸken (target) DaÄŸÄ±lÄ±mÄ±:")
print(df["target"].value_counts())

# Hedef deÄŸiÅŸkenin grafiÄŸi
sns.countplot(x='target', data=df, palette='Set2')
plt.title("Kalp HastalÄ±ÄŸÄ± DaÄŸÄ±lÄ±mÄ± (0 = Yok, 1 = Var)")
plt.xlabel("Kalp HastalÄ±ÄŸÄ±")
plt.ylabel("KiÅŸi SayÄ±sÄ±")
plt.tight_layout()
plt.savefig("results/target_dagilimi.png")
plt.show()


# ===============================
# ğŸ§¼ 5. AykÄ±rÄ± DeÄŸer Analizi
# ===============================
# Kutu grafikleriyle aykÄ±rÄ± deÄŸerleri gÃ¶rselleÅŸtirme
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_cols.remove('target')  # Hedef sÃ¼tunu Ã§Ä±kart

for col in numerical_cols:
    plt.figure(figsize=(6, 3))
    sns.boxplot(x=df[col])
    plt.title(f"{col} - AykÄ±rÄ± DeÄŸer Analizi")
    plt.show()

# ===============================
# ğŸ“Š 6. Korelasyon Matrisi
# ===============================
plt.figure(figsize=(10, 8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm')
plt.title("Ã–znitelikler ArasÄ± Korelasyon Matrisi")
plt.show()

# ===============================
# ğŸ” 7. Encoding (Gerekirse)
# ===============================
# Bu veri setinde tÃ¼m sÃ¼tunlar sayÄ±sal olduÄŸu iÃ§in encoding gerekmez.
# Ama ileride baÅŸka veri setleriyle Ã§alÄ±ÅŸÄ±rsan ÅŸu ÅŸekilde yapÄ±labilir:
# df = pd.get_dummies(df, columns=['categorical_column'])

# ===============================
# ğŸ“ 8. Veri Ã–lÃ§eklendirme
# ===============================
X = df.drop('target', axis=1)
y = df['target']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ===============================
# ğŸ¯ 9. EÄŸitim ve Test Verisi AyÄ±rma
# ===============================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print("âœ… EÄŸitim ve test verisi baÅŸarÄ±yla hazÄ±rlandÄ±!")


# ===============================
# ğŸ“Š 10. Histogramlar ve DaÄŸÄ±lÄ±m Grafikleri
# ===============================

#for col in numerical_cols:
    #plt.figure(figsize=(6, 3))
    #sns.histplot(df[col], kde=True, bins=30)
    #plt.title(f"{col} - Histogram")
    #plt.show()

# KlasÃ¶r oluÅŸtur (grafikler ayrÄ± klasÃ¶rde saklansÄ±n)
os.makedirs("results/grafikler", exist_ok=True)


# TÃ¼m sayÄ±sal sÃ¼tunlarÄ± + yeni eklenen sÃ¼tunlarÄ± listele
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_cols.remove('target')  # Hedef deÄŸiÅŸken Ã§Ä±karÄ±lsÄ±n

for col in numerical_cols:
    # Boxplot (AykÄ±rÄ± DeÄŸer)
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(f"{col.upper()} - AykÄ±rÄ± DeÄŸer Analizi", fontsize=12, fontweight='bold')
    plt.xlabel(col, fontsize=11)
    plt.grid(axis='x', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(f"results/grafikler/{col}_boxplot.png")
    #plt.show()
    plt.close()

    # Histogram (AykÄ±rÄ± DeÄŸer)
    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], bins=30, kde=True, color='lightcoral')
    plt.title(f"{col.upper()} - Histogram", fontsize=12, fontweight='bold')
    plt.xlabel(col, fontsize=11)
    plt.ylabel("Frekans", fontsize=11)
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(f"results/grafikler/{col}_histogram.png")
    #plt.show()
    plt.close()

# ===============================
# ğŸ” 11. Korelasyonu YÃ¼ksek Ã–zelliklerin Tespiti ve KaldÄ±rÄ±lmasÄ±
# ===============================
# Korelasyon eÅŸiÄŸi belirleyelim (Ã¶rneÄŸin 0.9)
correlation_matrix = df.corr().abs()
upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# YÃ¼ksek korelasyonlu sÃ¼tunlarÄ± tespit et
to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]
print(f"\nğŸ—‘ï¸ Korelasyonu yÃ¼ksek olduÄŸu iÃ§in Ã§Ä±karÄ±lacak sÃ¼tunlar: {to_drop}")

# 1. Korelasyonu yÃ¼ksek sÃ¼tunlar varsa aÃ§Ä±klama
if len(to_drop) == 0:
    print("â„¹ï¸ 0.9 Ã¼zerinde korelasyona sahip hiÃ§bir sÃ¼tun bulunamadÄ±. Bu nedenle Ã§Ä±karÄ±lacak sÃ¼tun yok.")
else:
    print(f"Ã‡Ä±karÄ±lan sÃ¼tunlar: {to_drop}")


# Veri setinden Ã§Ä±kar
df.drop(columns=to_drop, inplace=True)

# ===============================
# ğŸ¯ 12. GÃ¼ncel X ve y
# ===============================
X = df.drop("target", axis=1)
y = df["target"]

# Ã–lÃ§eklendirme
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# EÄŸitim/test bÃ¶lÃ¼nmesi
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)


# ===============================
# ğŸ¤– 13. Modelleme ve Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±
# ===============================


# KullanÄ±lacak modellerin listesi
models = {
    #"Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(),
    "Support Vector Machine": SVC(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
     "Gradient Boosting": GradientBoostingClassifier()
    #"MLP Classifier": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, early_stopping=True, random_state=42)
}

# Her model iÃ§in eÄŸitim, tahmin ve deÄŸerlendirme
for name, model in models.items():
    print(f"\nğŸ“Œ Model: {name}")
    
    # Modeli eÄŸit
    model.fit(X_train, y_train)
    
    # Test verisiyle tahmin yap
    y_pred = model.predict(X_test)
    
    # Accuracy (doÄŸruluk) oranÄ±
    acc = accuracy_score(y_test, y_pred)
    print(f"âœ… Accuracy: {acc:.4f}")
    
    # DetaylÄ± sÄ±nÄ±flandÄ±rma metrikleri
    print("ğŸ§¾ Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix gÃ¶rselleÅŸtirme
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Tahmin")
    plt.ylabel("GerÃ§ek")
    plt.show()

 # ===============================
# ğŸ”§ 14. Random Forest Optimizasyonu
# ===============================

from sklearn.model_selection import GridSearchCV

# Hiperparametre aralÄ±klarÄ±
param_grid_rf = {
    'n_estimators': [50, 100, 200],          # AÄŸaÃ§ sayÄ±sÄ±
    'max_depth': [None, 5, 10, 20],          # AÄŸaÃ§ derinliÄŸi
    'min_samples_split': [2, 5, 10],         # Dal bÃ¶lme minimum veri sayÄ±sÄ±
    'min_samples_leaf': [1, 2, 4]            # Yaprakta minimum Ã¶rnek
}

# GridSearchCV objesi oluÅŸtur
grid_rf = GridSearchCV(
    estimator=RandomForestClassifier(),
    param_grid=param_grid_rf,
    cv=5,                        # 5-fold cross validation
    scoring='accuracy',
    n_jobs=-1,                   # TÃ¼m Ã§ekirdekleri kullan
    verbose=1
)

# Modeli eÄŸit
grid_rf.fit(X_train, y_train)

# En iyi parametreler ve skor
print("\nğŸŒŸ En iyi parametreler (Random Forest):")
print(grid_rf.best_params_)

print(f"ğŸ† En iyi doÄŸruluk (CV): {grid_rf.best_score_:.4f}")

# Test verisiyle final tahmin
best_rf = grid_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test)

# Performans sonuÃ§larÄ±
print("\nâœ… Test Seti SonuÃ§larÄ± (Optimize EdilmiÅŸ Random Forest):")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt="d", cmap="Greens")
plt.title("Optimize EdilmiÅŸ Random Forest - Confusion Matrix")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.savefig("results/random_forest_optimizasyon.png")
plt.show()

# ===============================
# ğŸ”§15. Support Vector Machine (SVM) Optimizasyonu
# ===============================
from sklearn.pipeline import Pipeline

# SVM iÃ§in parametre aralÄ±klarÄ±
param_grid_svm = {
    'svc__C': [0.1, 1, 10],
    'svc__kernel': ['linear', 'rbf', 'poly'],
    'svc__gamma': ['scale', 'auto']
}

# Pipeline oluÅŸturuyoruz: scaler + SVM birlikte optimize edilecek
pipe_svm = Pipeline([
    ('scaler', StandardScaler()),  # Ekstra garanti iÃ§in tekrar Ã¶lÃ§eklendirme
    ('svc', SVC())
])

# GridSearchCV uygulama
grid_svm = GridSearchCV(
    estimator=pipe_svm,
    param_grid=param_grid_svm,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# EÄŸit
grid_svm.fit(X_train, y_train)

# En iyi sonuÃ§lar
print("\nğŸŒŸ En iyi parametreler (SVM):")
print(grid_svm.best_params_)

print(f"ğŸ† En iyi doÄŸruluk (CV): {grid_svm.best_score_:.4f}")

# Test setinde deÄŸerlendirme
best_svm = grid_svm.best_estimator_
y_pred_svm = best_svm.predict(X_test)

print("\nâœ… Test Seti SonuÃ§larÄ± (Optimize EdilmiÅŸ SVM):")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}")
print(classification_report(y_test, y_pred_svm))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm, annot=True, fmt="d", cmap="Oranges")
plt.title("Optimize EdilmiÅŸ SVM - Confusion Matrix")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.savefig("results/svm_optimizasyon.png")
plt.show()

# ===============================
# ğŸ”§ 16. K-Nearest Neighbors (KNN) Optimizasyonu
# ===============================
# KÃ¼tÃ¼phane zaten eklenmiÅŸ olmalÄ±: from sklearn.neighbors import KNeighborsClassifier

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

grid_knn = GridSearchCV(
    estimator=KNeighborsClassifier(),
    param_grid=param_grid_knn,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_knn.fit(X_train, y_train)

# En iyi sonuÃ§lar
print("\nğŸŒŸ En iyi parametreler (KNN):")
print(grid_knn.best_params_)

print(f"ğŸ† En iyi doÄŸruluk (CV): {grid_knn.best_score_:.4f}")

# Test seti deÄŸerlendirmesi
best_knn = grid_knn.best_estimator_
y_pred_knn = best_knn.predict(X_test)

print("\nâœ… Test Seti SonuÃ§larÄ± (Optimize EdilmiÅŸ KNN):")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}")
print(classification_report(y_test, y_pred_knn))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_knn)
sns.heatmap(cm, annot=True, fmt="d", cmap="Purples")
plt.title("Optimize EdilmiÅŸ KNN - Confusion Matrix")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.savefig("results/knn_optimizasyon.png")
plt.show()

# ===============================
# ğŸ”§ 17. Gradient Boosting Optimizasyonu
# ===============================

param_grid_gb = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

grid_gb = GridSearchCV(
    estimator=GradientBoostingClassifier(),
    param_grid=param_grid_gb,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_gb.fit(X_train, y_train)

print("\nğŸŒŸ En iyi parametreler (Gradient Boosting):")
print(grid_gb.best_params_)

print(f"ğŸ† En iyi doÄŸruluk (CV): {grid_gb.best_score_:.4f}")

# Test deÄŸerlendirmesi
best_gb = grid_gb.best_estimator_
y_pred_gb = best_gb.predict(X_test)

print("\nâœ… Test Seti SonuÃ§larÄ± (Optimize EdilmiÅŸ Gradient Boosting):")
print(f"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}")
print(classification_report(y_test, y_pred_gb))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_gb)
sns.heatmap(cm, annot=True, fmt="d", cmap="BuGn")
plt.title("Optimize EdilmiÅŸ Gradient Boosting - Confusion Matrix")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.show()

# ===============================
# ğŸ“ˆ 18. Ã–zellik Ã–nem GrafiÄŸi (Random Forest ile)
# ===============================

# Model tekrar eÄŸitiliyor Ã§Ã¼nkÃ¼ tÃ¼m X'i kullanmak istiyoruz
rf = RandomForestClassifier(random_state=42)
rf.fit(X_scaled, y)

feature_importances = rf.feature_importances_
features = X.columns

importance_df = pd.DataFrame({
    'Ã–zellik': features,
    'Ã–nem': feature_importances
}).sort_values(by='Ã–nem', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Ã–nem', y='Ã–zellik', data=importance_df, color="skyblue")
plt.title(" Ã–zellik Ã–nem GrafiÄŸi (Random Forest)")
plt.xlabel("Ã–nem Derecesi")
plt.ylabel("Ã–zellikler")
plt.tight_layout()
plt.savefig("results/ozellik_onem_grafigi.png")
plt.show()








